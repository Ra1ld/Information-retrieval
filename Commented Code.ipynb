{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0de3d27c-1328-4268-8c1d-2d75a3faf1c9",
   "metadata": {},
   "source": [
    "# ***Τροπος Επεξηγησης Κωδικα***\n",
    "**Η επεξηγηση του κωδικα εχει γινει με τετοια μορφη που να βγαζει νοημα απο αποψη εκτελεσης.** \n",
    "\n",
    "Δεν εχω παει απλα να πω τι κανει ενα block code, αλλα πηγαινω νοηματικα απο σημειο σε σημειο που πιστευω οτι λειτουργουν συνεργατικα. Επομενως υπαρχει περιπτωση να κανω \"μεταπηδημα\" στις εξηγησεις μου , αλλα πιστευω οτι αυτο ειναι το σωστο εδω λογω του τροπου εκτελεσης της python.\n",
    "\n",
    "Θεωρω οτι θα ηταν κακο να παω να εξηγω ενα τμημα κωδικα πριν καν φανει ο τροπος εκτελεσης του αργοτερα.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d286909-a8f1-46eb-aee3-361fb2c419b5",
   "metadata": {},
   "source": [
    "# ***Φορτωση Βιβλιοθηκων***\n",
    "Φορτώνουμε στο πρόγραμμα τις εξής βιβλιοθήκες\n",
    "1)  requests\n",
    "2)  BeautifulSoup\n",
    "3)  re\n",
    "\n",
    "\n",
    "\n",
    "### 1. Requests\n",
    "**Είναι βιβλιοθήκη η οποία  χειρίζεται την επικοινωνία  του υπολογιστή  και του http server**\n",
    "\n",
    "Συγκεκριμένα μέσω του κώδικα μας κάνουμε αιτήματα (  χρησιμοποιώντας τις συναρτήσεις αυτής της βιβλιοθήκης )  στον http server προκειμένου να συνδεθούμε σε αυτόν.\n",
    "\n",
    "### 2. BeatifulSoup\n",
    "**Eίναι η βιβλιοθήκη η οποία  έχει έτοιμες συναρτήσεις  για την επεξεργασία και περιήγηση σε html αρχαία**\n",
    "\n",
    "Aυτή η βιβλιοθήκη βρίσκεται στο πακέτο bs4 (  από τον οποίο την κάνουμε import )\n",
    "\n",
    "Εξου και  εντολή:  `from bs4 import beautiful soup`\n",
    "\n",
    "### 3. re\n",
    "**Είναι βιβλιοθήκη η οποία μας επιτρέπει να εισάγουμε κανονικές εκφράσεις στο πρόγραμμα μας,  και να κάνουμε διάφορες λειτουργίες χρησιμοποιώντας κανονικές εκφράσεις**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c49c4cb-5173-4c72-9829-844a5684ced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6ecbe8-7990-45fb-955f-63474a85f0ba",
   "metadata": {},
   "source": [
    "# ***Δηλωση Συναρτησεων***\n",
    "Ενας python κωδικας εκτελείται από πάνω προς τα κάτω, αρχιζοντας απο την αρχη του αρχειου.\n",
    "Στην δικια μας περιπτωση αυτο σημαινει οτι αφού κάνει import τις προηγούμενες βιβλιοθήκες, ο κώδικας θα συναντήσει τις τρεις παρακάτω συναρτήσεις: \n",
    "\n",
    "1) fetch_wikipedia_page\n",
    "2) extract_links\n",
    "3) crawl_wikipedia\n",
    "\n",
    "Αυτές οι συναρτήσεις έχουν σώμα,  αλλά δεν μπορούν να εκτελεστούνε( απευθείας εκεινη την ωρα )  χωρίς να καλεστούνε πρωτα.\n",
    "Επομένως ο compiler με το που φτάσει σε αυτές, τις αποθηκεύει στην μνήμη έτσι ώστε να μπορούν να κληθούν αργότερα στο προγραμμα.\n",
    "\n",
    "Αυτό είναι απαραίτητο να γίνει διότι αυτές συναρτήσεις  θα καλεστούνε  πολύ σύντομα  και ο compiler θα πρέπει να \"γνωρίζει\" την ύπαρξή τους, πριν εκτελεστουν."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d92d63-84d2-4743-a5ec-9b969fb68cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_wikipedia_page(url):\n",
    "    \"\"\"\n",
    "    Fetches and parses a Wikipedia page given its URL.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise HTTPError for bad requests (4xx and 5xx)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        return soup\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching URL {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_links(soup):\n",
    "    \"\"\"\n",
    "    Extracts and returns a list of valid Wikipedia links from a page.\n",
    "    \"\"\"\n",
    "    base_url = \"https://en.wikipedia.org\"\n",
    "    links = []\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "        # Filter for Wikipedia article links\n",
    "        if href.startswith('/wiki/') and not re.search(r':', href):\n",
    "            full_url = base_url + href\n",
    "            links.append(full_url)\n",
    "    return links\n",
    "\n",
    "def crawl_wikipedia(start_url, max_depth=2, max_pages=10):\n",
    "    \"\"\"\n",
    "    Crawls Wikipedia starting from `start_url` up to `max_depth` levels.\n",
    "    \"\"\"\n",
    "    parsed_paragraph = []\n",
    "    visited = set()\n",
    "    to_visit = [(start_url, 0)]  # (URL, depth)\n",
    "    result = []\n",
    "\n",
    "    while to_visit and len(result) < max_pages:\n",
    "        current_url, depth = to_visit.pop(0)\n",
    "        if depth > max_depth or current_url in visited:\n",
    "            continue\n",
    "\n",
    "        print(f\"Visiting: {current_url} at depth {depth}\")\n",
    "        soup = fetch_wikipedia_page(current_url)\n",
    "        \n",
    "        if soup is None:\n",
    "            continue\n",
    "\n",
    "        paragraphs = soup.find_all('p')\n",
    "        #append in one list\n",
    "        text = [p.text.strip() for p in paragraphs] \n",
    "        parsed_paragraph.append(\" \".join(text))\n",
    "\n",
    "        visited.add(current_url)\n",
    "        result.append(current_url)\n",
    "\n",
    "        links = extract_links(soup)\n",
    "        for link in links:\n",
    "            if link not in visited:\n",
    "                to_visit.append((link, depth + 1))\n",
    "    \n",
    "    return (result, parsed_paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc0fa7b-056e-4bb4-a4c3-ccd77d79ea69",
   "metadata": {},
   "source": [
    "# ***Βημα 1***\n",
    "Μετά το importing και τις δηλώσεις,  φτάνουμε στο πρώτο σημείο του κώδικα στο οποίο αρχίζει να φαίνεται η λειτουργία του web crawler( βημα 1 )\n",
    "\n",
    "\n",
    "### 1) start_url\n",
    "Στην μεταβλητή start_url  εισάγουμε το URL της σελίδας που θέλουμε να αρχίσουμε την διαδικασια του web crawling και σε αυτή την περίπτωση επέλεξα να αρχίσω από μία \"σχετική του θέματος\" σελίδα του wikipedia.\n",
    "\n",
    "### 2) καλεσμα της συναρτησης crawl_wikipedia\n",
    "Καλούμε τη συνάρτηση \"crawl_wikipedia\"  η οποία δέχεται παραμέτρους: \n",
    "\n",
    "1.  Url της αφετηριας\n",
    "2.  μεταβλητή max_depth(  η οποία  δημιουργείται εκείνη την ωρα και της εκχωρείται τιμή ταυτόχρονα μεσα στην παραμετρο της συναρητησης )\n",
    "3.  μεταβλητή max_pages(  η οποία  δημιουργείται εκείνη την ωρα και της εκχωρείται τιμή ταυτόχρονα μεσα στην παραμετρο της συναρητησης )\n",
    "\n",
    "\n",
    "Η συνάντηση αυτή η επιστρέφει ένα tuple τιμων(  πολλές τιμές που περιλαμβάνονται σε μία δομή ) που καθε μια αποθηκευεται στις μεταβλητές crawled_pages/ paragraphs (  οι οποίες δημιουργούνται  εκείνη την ώρα  ),  όπου ο τύπος δεδομένων τους θα είναι ίδιος με την επιστρεφόμενη τιμή σε αυτές.\n",
    "\n",
    "Mετά το κάλεσμα της συνάρτησης η μεταβλητή crawled pages  θα έχει το πλήθος( μια λιστα ) των url που συλλέχθηκαν από την συνάρτηση,  το οποίο θα χρησιμοποιηθεί αμέσως μετά προκειμένου να περιηγηθούμε την  λίστα και να εμφανίσουμε  ποια URL συλλέχτηκαν.\n",
    "Επειτα μεσω for loop θα αρχικοποιησουμε μια μεταβλητη pages ( η οποια θα παρει τις τιμες της λιστας, κανοντας την να εχει τον ιδιο τυπο δεδομενων ) και θα εμφανιζουμε το περιεχομενο της για καθε καινουργιο url της λιστας\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b22150-37b4-48a2-bab5-768233dff2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_url = \"https://en.wikipedia.org/wiki/Web_scraping\"\n",
    "(crawled_pages, paragraphs) = crawl_wikipedia(start_url, max_depth=2, max_pages=20)\n",
    "print(\"\\nCrawled Pages:\")\n",
    "for page in crawled_pages:\n",
    "    print(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9af02c-e2a0-4487-81a1-2286a565c0a9",
   "metadata": {},
   "source": [
    "# ***Εκτελεση συναρτησης crawl_wikipedia***\n",
    "\n",
    "### `parsed_paragraph = []`\n",
    "Φτιάχνουμε μια κενή λίστα με ονομα parsed_paragraph  η οποία ***σε καθε indexing της***, θα εχει ως περιεχομενο ολες τις παραγραφους καθε\n",
    "HTML αρχειου.\n",
    "Δηλάδη καθε indexing θα αντιστοιχει σε ενα HTML αρχειό και το περιεχομενο θα ειναι οι παραγραφοι αυτων `<p>`\n",
    "\n",
    "### `paragraphs = soup.find_all('p')`\n",
    "Η λειτουργία της συνάρτησης είναι  να αναζητεί ολα τα tags `<p>` στο html αρχειο που βρισκομαστε εκεινη την ωρα.\n",
    "Η επιστρεφόμενη τιμή είναι μία λίστα αντικειμένων,  όπου κάθε αντικείμενο ειναι μια παράγραφος.\n",
    "Αυτή η εντολή επειδή βρίσκεται μέσα σε καταλληλο while loop(βλεπε παρακατω) και εκτελείται επαναληπτικά για καθε html αρχειο, το περιεχομενο της θα αλλαζει καθε φορα αναλογως του html αρχειου που βρισκομαστε.\n",
    "\n",
    "### `text = [p.text.strip() for p in paragraphs]`\n",
    "\n",
    "Αυτη τη εντολη θα μπορουσε να γραφτει και ως εξης:\n",
    "```\n",
    "text = []\n",
    "for p in paragraphs\n",
    "text.apppend(p.text.strip())\n",
    "```\n",
    "Στην αρχικη συτναξη, το κανω σε μια εντολη με την μια. \n",
    "Η ολη ιδεα ειναι οτι κανουμε εναν βροχο οπου η μεταβλητη p κανει κυκλο και αποκτα σιγα σιγα καθε τιμη(τα αντικειμενα) της λιστας **paragraphs**, αλλα ταυτοχρονα το αποτελεσμα αποθηκευεται σε μια καινουργια προσωρινη λιστα με ονομα **text**...δηλαδη αντι να δηλωσω πρωτα την λιστα και μετα να κανω την for και append σε αυτη...υπαρχει συνακτικο που με αφηνει να το κανω ολο με την μια.\n",
    "\n",
    "Απο εκει και περα, χρησιμοποιουμε τις εξης μεθοδους\n",
    "1. text: μεθοδος της beatiful soup για αφαιρεση των tags απο ενα κειμενο...δηλαδη για να παρουμε μονο το περιεχομενο των tagg\n",
    "2. strip: default μεθοδος της python για αφαιρεση κενων, tabs, newlines κτλ...γενικα για μορφοποιηση ενος κειμενου\n",
    "\n",
    "### `parsed_paragraph.append(\" \".join(text))`\n",
    "\n",
    "Εφοσον εχει δημιουργηθει η προσωρινη λιστα **text** που θα ειναι και καταλληλα μορφοποιημενη( χωρις html tags και ασχετα κενα ), την κανουμε append πλεον στην αρχικη λιστα **parsed_paragraph**  που θα κρατησει ολες τις μορφοποιημενες παραγραφους απο ολα τα html\n",
    "Υπενθυμιζω οτι το περιεχομενο της  λιστας text γινεται overide σε καθε επαναληψη της while, και το περιεχομεο της μετα γινεται join( μαζι με ενα κενο ) στην αρχικη λιστα. To overide δεν μας πειραζει( ισα ισα το αντιθετο ) διοτι ο σκοπος αυτης ειναι απλα να κρατησει εκεινη την ωρα τα αντικειμενα του html αρχειου σε μορφοποιημενη μορφη.\n",
    "\n",
    "Επομενως, κανουμε join το μοφροποιημενο αντικειμενο με ενα κενο απο πισω( ετσι ωστε οταν γινει append να ξεχωρισει απο τα προηγουμενα αντικειμενα της αρχικης λιστας) και επειτα ολο αυτο το κανουμε append.\n",
    "\n",
    "### `visited = set()` και `visited.add(current_url)`\n",
    "\n",
    "**Αυτές οι εντολές λειτουργούν συνεργατικά μεταξύ τους,  και ειναι σημαντικο να εξηγηθούν  σαν ζευγος(  παρόλο που δεν βρίσκονται τοσο κοντά μεταξύ τους στον κωδικα )**\n",
    "\n",
    "\n",
    "Η μεθοδος set() ειναι ενα ειδικό αντικείμενο  στην python που μας επιτρέπει να αποθηκεύσουμε τιμές(σε αυτο)  τις οποίες δεν θέλουμε να επαναληφθούν στο μέλλον, διοτι το αντικειμενο set αφαιρει αυτοματα τις εκχωρησεις που υπαρχουν ηδη μεσα σε αυτο. **Επομένως και η μεταβλητή visited  θα είναι τύπου (  ειδικό αντικειμένου ) set**\n",
    "\n",
    "\n",
    "\n",
    "Στην μεταβλητή visited (  τυπου set )  θα προσθέτουμε τα url των HTML τα οποία επισκεπτόμαστε κάθε φορά( λογω while loop),  έτσι ώστε να σιγουρευτούμε ότι σε μελλοντική επανάληψη δεν θα τα επισκεφτούμε.\n",
    "\n",
    "\n",
    "\n",
    "Αυτός ο έλεγχος  γίνεται μέσα στη while,  χρησιμοποιώντας μια συνθήκη if που οποία ελέγχει  αν το τωρινό url (   αποθηκευμένο στη μεταβλητή current_url ) βρίσκεται μέσα σε αυτή τη μεταβλητή visted( τυπου set ). \n",
    "`if depth > max_depth or current_url in visited:`\n",
    "`continue`\n",
    "        \n",
    "        \n",
    "\n",
    "Αν το url  βρίσκεται όντως μέσα στη μεταβλητή visited, τοτε θα εκτελεστει μια εντολη continue αγνοωντας το συγκεκριμενο βημα επαναληψης( διοτι δεν θελουμε να ελεγξουμε κατι που εχει ελεγχτει ηδη )\n",
    "\n",
    "### `to_visit = [(start_url, 0)]` και `current_url, depth = to_visit.pop(0)`\n",
    "\n",
    "Αρχικα φτιάχνουμε μία λίστα με όνομα **to_visit**  στην οποία θα αποθηκεύουμε  δύο διαφορετικές τιμές για κάθε index του.\n",
    "\n",
    "1. το url που θελουμε να επεξεργαστουμε( στην αρχη το αρχικοποιουμε με το αρχικο url )\n",
    "2. το βαθος του οσο αναφορα την \"σειρα\" του σε σχεση με τα υπολοιπα που εχει επεξεργαστει ο crawler\n",
    "\n",
    "Σε κάθε επανάληψη κάνουμε pop από την λίστα  το πρώτο στοιχείο της( το μηδενικο ) και αποθηκεύουμε το url στην  μεταβλητή current_url και το βαθος στην μεταβλητη depth. Aυτό μπορούμε να το κάνουμε διότι η λίστα λειτουργεί ως tuple(  αποθηκεύει 2 τιμές ),  και υπάρχουν επιτρέπει την εκχώρηση ενός tuple σε δύο διαφορετικές μεταπτές ταυτόχρονα.\n",
    "\n",
    "### **FIFO και λογως χρησης pop(0)**\n",
    "Ο λόγος που κάνουμε pop το πρώτο στοιχείο κάθε φορά( το μηδεν ) από την λίστα είναι γιατί ακολουθούμε τη λογική FIFO.\n",
    "\n",
    "Ακολουθούμε τη λογική fifo διοτι ο crawler σαν συμπεριφορά  λειτουργεί με την έννοια του αλγοριθμου BFS (  Breadth-First Search ),  όπου  αναζητούμε τα δεδομένα με βάση το βάθος τους ( το επιπεδο βαθους...δηλαδη αναζητουμε πρωτα ολο το επιπεδο και μετα παμε σε μεγαλυτερο βαθος ).\n",
    "\n",
    "Επειδή web crawler λειτουργεί με την ίδια λογική σαν θεωρια,  και η python  αυτόματα μας επιτρέπει να κάνουμε pop το **ΤΩΡΙΝΑ** πρωτο στοιχειο απο την λιστα γραφοντας pop(0) ακομα και αν αυτο δεν ηταν το 0 αρχικα...βγάζει απόλυτο νόημα να λειτουργήσουμε με τη λογική FIFO. \n",
    "\n",
    "\n",
    "### **Πως ενημερωνεται η μεταβλητη depth**\n",
    "\n",
    "Η μεταβλητή depth  ενημερώνονται μέσω αυτό των 2 εντολών\n",
    "\n",
    "1. to_visit.append((link, depth + 1))\n",
    "\n",
    "2. current_url, depth = to_visit.pop(0)\n",
    "\n",
    "Στην ουσία όταν φτάνουμε στην εντολή `to_visit.append((link, depth + 1))`  αποθηκεύουμε στην λίστα **to_visit**  το επόμενο link προς επεξεργασία αλλά και αυξάνουμε το βάθος της μεταβλητής depth κατα 1. Εδω να επισημανουμε οτι κατα την επισκεψη σε καποιο url, μπορει να εχει στο επιπεδο του πολλαπλα url τα οποια θα πρεπει να προσπελαστουν μεσω της while...που σημαινει αυτα θα εχουν το ιδιο επιπεδο( αρα η τιμη depth για αυτα θα παραμεινει ιδια, διοτι το αρχικο depth του γονεα δεν θα αλλαξει). \n",
    "\n",
    "**Θα παμε σε επομενο επιπεδο depth μονο αν καποιο απο αυτα τα \"πολλαπλα\" url εχουν και αυτα με την σειρα τους καποια url για επεξεργασια.**\n",
    "\n",
    "Αυτή η επαναληπτική τη διαδικασία  κάνει το depth  να αυξάνεται ολόνα και περισσότερο σαν τιμή( οσο πιο βαθια παει ο web crawler )  με αποτέλεσμα κάποια στιγμή να φτάσει  την τιμή της μεταβλητής max_depth( οπως και αν την αρχικοποιησουμε ) \n",
    "\n",
    "\n",
    "\n",
    "Για παραδειγμα \n",
    "\n",
    "το depth στην αρχη εχει την τιμη 0\n",
    "\n",
    "Επειτα αν παει στο επομενο url θα εχει την τιμη 1, αλλα σε αυτο το url μπορει να υπαρχουν 5+ url σε αυτο το επιπεδο τα οποια θα κανει search. \n",
    "\n",
    "\n",
    "ΑΦΟΥ ΤΕΛΕΙΩΣΕ ΜΕ ΑΥΤΑ( ΛΟΓΙΚΗ BFS ) ,τοτε θα ψαξει να δει αν στα επιμερους url που εψαξε, υπαρχουν και αλλα που πρεπει να ψαξει ( υποδικνυοντας οτι θα παει σε πιο βαθυ επιπεδο )\n",
    "\n",
    "\n",
    "\n",
    "### `print(f\"Visiting: {current_url} at depth {depth}\")`\n",
    "\n",
    "αυτή η εντολή εκτελεί επαναληπτικά(  διότι βρίσκεται μέσα σε while loop)  την τωρινή κατάσταση του crawler.\n",
    "\n",
    "Δηλαδή το url  που επεξεργάζεται  και το βάθος του(  έχω προαναφερθεί σε αυτό λεπτομερως ακριβως απο πανω )\n",
    "\n",
    "### `soup = fetch_wikipedia_page(current_url)`\n",
    "\n",
    "Η εντολή αυτή εκτελεί την συνάρτηση fetch_wikipedia_page()  και παίρνει ως παράμετρο το url που επεξεργαζόμαστε  εκείνη τη στιγμή.\n",
    "\n",
    "Κατά την εκτέλεση αυτής της συνάρτησης,  γίνεται έντονη χρήση της βιβλιοθήκης request  ως μία απόπειρα να συνδεθούμε με τον http server του συγκεκριμένου url που πήραμε ως παράμετρο.\n",
    "\n",
    "Αυτή η απόπειρα  βρίσκεται μέσα σε **try...except**  έτσι ώστε  στην περίπτωση που αποτύχει  το request(δηλαδη να αποτυχει το try) ,  να μπουμε στο except και να επιστρέψουμε ένα ειδικο αντικειμενο **None**  πίσω στο σημείο που καλέστηκε(  διότι σε μία τέτοια περίπτωση  θα θέλαμε να κάνουμε continue  στο επόμενο βήμα της  αρχικής επανάληψης )."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dd5b18-0e60-4eec-b06e-3d15cbb3f699",
   "metadata": {},
   "source": [
    "# ***Φορτωση Βιβλιοθηκης pandas***\n",
    "\n",
    "Φορτώνουμε την βιβλιοθήκη εν ονόματι **pandas**  και της αλλαζουμε το ονομα τοπικά στον κώδικα μας  ως **pd**  προκειμένου να είναι πιο  εύκολη  η πρόσβαση στις μεθόδους της.\n",
    "\n",
    "Για να μην πολυλογώ για την θεωρητική επεξήγηση, την εχω γράψει στην αναφορά αρκετά λεπτομερώς"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dfce6f-db30-4162-9985-3c19b5fb4936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba3c988-c608-405e-b2e3-29e083c16eca",
   "metadata": {},
   "source": [
    "# ***Δημιουργια DataFrame***\n",
    "\n",
    "Εδώ δημιουργούμε ένα dataframe με ονομα **df**. Tο Dataframe είναι ένα  ειδικό αντικείμενο της βιβλιοθήκης **pandas** που  μοιάζει με ένα δύσδιάστατο array. Οι μέθοδοι της βιβλιοθήκης pandas,  μας επιτρέπουν να επεξεργαστούμε και να εμφανίσουμε το  περιεχόμενο  με διαφορετικούς τρόπους.\n",
    "\n",
    "Στην δικιά μας περίπτωση  φτιάχνουμε δύο στήλες με ονομα:\n",
    "1. URL: αποθηκευουμε ολα τα url που βρηκε η προηγουμενη συναρτηση\n",
    "2. Text: Οι παραγραφοι απο καθε ενα απο αυτα τα URL\n",
    "\n",
    "Επομενως το DataFrame μας σε καθε γραμμη του θα εχει 1 στηλη για τα URL και μια που περιεχει τις παραγραφους αυτων\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e9e0d0-991a-488d-a2cd-03319d439013",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"URL\": crawled_pages,\n",
    "    \"Text\": paragraphs\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35b8b6b-72cb-4253-98fe-b9582404256b",
   "metadata": {},
   "source": [
    "# ***Δημιουργια csv με βαση το DataFrame***\n",
    "\n",
    "Η μέθοδος **to_csv**  δημιουργεί ενα csv αρχειο με βαση το dataframe **df**\n",
    "\n",
    "Tο όνομα αυτού του αρχείου  το ορίζουμε ως **wikipedia_data.csv**\n",
    "\n",
    "Επειδή το dataframe  κάνε αριθμηση στις γραμμές,  θέλουμε να βάλουμε την παράμετρο **index=False**  έτσι ώστε αυτή η αριθμηση να μην αποτελεί ξεχωριστή  στηλη στο csv(  επηρεάζοντας έτσι τη συμπεριφορά του αρχείου με το προγραμμα )\n",
    "\n",
    "Το encoding='utf-8' διασφαλίζει ότι το αρχείο CSV αποθηκεύεται με την κωδικοποίηση UTF-8, η οποία υποστηρίζει ειδικούς χαρακτήρες\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e354cc-bbb8-4005-bf45-02dc6cc99e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('wikipedia_data.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e04b4a-528c-4e9d-9b83-c7438ebfa381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your CSV file\n",
    "csv_file_path = \"wikipedia_data.csv\"\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "data = pd.read_csv(csv_file_path, header=0)\n",
    "\n",
    "# Display the loaded data\n",
    "data.head()\n",
    "\n",
    "# Access the column as a list\n",
    "column_data = data['Text'].tolist() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7ccee9-146d-463a-b983-a0d05adb6473",
   "metadata": {},
   "source": [
    "# **Importing των βιβλιοθηκων για Tokenization και επεξεργασια κειμενου**\n",
    "\n",
    "Eχει γινει σημαντικη και λεπτομερη επεξηγηση στην αναφορα σχετικα με το τι ειναι το tokenization και πως λειτουργουν αυτες οι μεθοδοι/βιβλιοθηκες."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f543186-ecce-41cb-91ae-41342f523df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca43eec3-d02c-479b-b188-dbb4113e9359",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c7dd09-29d5-422e-94da-7fec66dc36d4",
   "metadata": {},
   "source": [
    "# ***Δηλωση της συναρτησης preprocess_text και λειτουργια της***\n",
    "\n",
    "Αυτη ειναι η συναρτηση που θα κανει την προεπεξεργασια του κειμενου...το tokenization.\n",
    "Δέχεται ως παράμετρο το text\n",
    "\n",
    "text => item => καθε γραμμη της λιστας column_data(εκτελειται επαναλπητικα η συναρτηση για καθε τιμη της λιστας),\n",
    "οπου στην ουσία είναι το item που αποκτα καθε τιμη επαναληπτικα της λιστας column_data \n",
    "\n",
    "1) `text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)`\n",
    "Χρησιμοποιεί τη μέθοδο re.sub από τη βιβλιοθήκη των Regular Expressions η οποία δέχεται ως είσοδο το text(  2η παράμετρος ) στην re.sub  και στην ουσία καθαρίζει ότι δεν είναι πεζο η κεφαλαίο λατινικό γράμμα ή κενο.\n",
    "\n",
    "`r\"[^a-zA-Z\\s]\"`\n",
    "To ^ οταν ειναι μεσα στις αγγυλες( στην αρχη ομως ) υποδικνυει οτι αν κατι δεν αρχιζει συμφωνα με την κανονικη εκγραφση που θα ακολουθησει, τοτε θα πρεπει να αντικατασταθει ( σε αυτη την περιπτωση εχουμε επιλεξει αντικατασταση με κενο )\n",
    "1. r\"\" Δηλωνει οτι προκειται για string\n",
    "2. Οι [] ορίζουν ένα σύνολο χαρακτήρων\n",
    "3. [a-z] σημαίνει οποιοδήποτε πεζό γράμμα από το 'a' έως το 'z' στην αλφαβητο\n",
    "4. [Α-Ζ] Όλα τα κεφαλαία γράμματα απο 'Α' εως 'Ζ'\n",
    "5. \\s σημαίνει whitespace\n",
    "\n",
    "2) `tokens = word_tokenize(text)`\n",
    "Εφοσον το text καθαρισε απο την προηγουμενη εντολη, τωρα μπορουμε να παμε να χωρισουμε την προταση σε λεξεις( δηλαδη σε ξεχωριστα tokens )\n",
    "Το αποτελεσμα της μεθοδου( με εισοδο την καθε καθαρισμενη πλεον παραγραφο ), αποθηκευεται  σε μια λιστα με ονομα tokens\n",
    "\n",
    "3) ``stop_words = set(stopwords.words(\"english\"))`` και  ``tokens = [word for word in tokens if word not in stop_words]``\n",
    "Stopwords ειναι κοινες λεξεις της αλφαβητου που δεν εχουν καποια ιδιαιτερη σημασια( ειδικα για εφαρμογες οπως web scraping ) και καλο ειναι να αφαιρουνται\n",
    "Η πρωτη εντολη `stopwords.words` ειναι μια μεθοδος που επιστρεφει μια λιστα απο stopwords( αναλογως της επιλογης της παραμετρου )... σε αυτη την περιπτωση περνουμε την λιστα και την κανουμε ενα set list( δηλαδη λιστα που δεν επιτρεπει την επαναληψη 2 ιδιων τιμων ) και την αποθηκευουμε στην μεταβλητη stop_words\n",
    "\n",
    "Επειτα τρεχουμε το ηδη υπαρχων περιεχομενο της λιστας **tokens** που ειχαμε φτιαξει προηγουμενως χρησιμοποιωντας μια for loop για καθε ενα στοιχειο της λιστας. Δηλαδη η μεταβλητη **word** θα λαμβανει καθε μια τιμη της λιστας **tokens** και εφοσον το στοιχειο της **word** εκεινη την ωρα δεν εχει καποια ομοιοτητα με την **set** λιστα με ονομα **stop_words** ( δηλαδη να μην ειναι stop_word) τοτε θα αποθηκευεται στην λιστα tokens.\n",
    "\n",
    "Σημαντικο οτι εδω η λιστα tokens γινεται overwrite\n",
    "\n",
    "4) `lemmatizer = WordNetLemmatizer()` και `tokens = [lemmatizer.lemmatize(word) for word in tokens]`\n",
    "Το lemmatization είναι η διαδικασία μετατροπής μιας λέξης στη βασική της μορφή (lemma), λαμβάνοντας υπόψη τη γραμματική της σημασία.\n",
    "Δηλαδη την λεξη **τρεξιμο** θα την μετατρεψει ως **τρεχω**\n",
    "\n",
    "Ο lemmatizer χρησιμοποιεί το WordNet ( το οποιο κατεβασαμε λιγο πιο πανω με την εντολη `nltk.download('wordnet')` ). Προκεται για ένα λεξικό με πληροφορίες για τις λέξεις ( μέρος του λόγου, λεξικό σημασίας )\n",
    "\n",
    "Αρχικα δημιουργουμε ενα lemmatizer\n",
    "\n",
    "Εφαρμοζουμε την μεθοδο **lemmatize** για καθε λεξη μεσα στην λιστα tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b4a8db-77b0-4d28-b892-41464d4a66ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Αφαίρεση ειδικών χαρακτήρων και αριθμών\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Μετατροπή σε πεζά\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "\n",
    "    # Αφαίρεση stop-words\n",
    "    stop_words = set(stopwords.words(\"english\"))  \n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5afb85-c783-4d24-b87a-a9a1accd027c",
   "metadata": {},
   "source": [
    "# **Preprocessing**\n",
    "\n",
    "1) `cleaned_text = []`\n",
    "Δημιουργείται μια κενή λίστα με όνομα cleaned_text\n",
    "\n",
    "2) `for item in column_data:`\n",
    "Το column_data περιέχει όλα τα δεδομένα της στήλης Text του DataFrame data, σε μορφή λίστας Python.\n",
    "\n",
    "Θυμιζω οτι η στηλη Text του DataFrame ειχε αποθηκευμενες ολες τις παραγραφους και η column data παιρνει αυτες τις παραγραφους για καθε γραμμη και τις ξανακανει σε λιστα python( απο csv που ηταν )\n",
    "\n",
    "Δηλαδη κάθε στοιχείο της λίστας αντιστοιχεί σε καθε στοιχειο της στήλης Text στο DataFrame.\n",
    "\n",
    "\n",
    "Το item παιρνει επαναληπτικα τις τιμες του column_data\n",
    "\n",
    "3) `cleaned_text.append(preprocess_text(item))`\n",
    "Για καθε τιμη( text ) της λιστας column_date( το item παρινει μια μια τις τιμες του column_date ), καλουμε την συναρτηση preprocess_text η οποια θα χειριστει την προεπεξεργασια(για τα επομενα βηματα)...δηλαδη εδω θα κανουμε tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80281120-d98d-41a9-88f8-c107e3cd8cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = []\n",
    "for item in column_data:\n",
    "    cleaned_text.append(preprocess_text(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53cab94-bf39-4afb-910b-1475b689f586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68e68e8-fba4-4595-9fe0-385016788fac",
   "metadata": {},
   "source": [
    "# ***Δηλωση της συναρτησης create_inverted_index και λειτουργεια της***\n",
    "\n",
    "**εχω εξηγησει εκτενως(στην αναφορα) τι ειναι ενα inverted index σε σχεση με μια απλη δομη δεδομενων**\n",
    "\n",
    "Η συναρτηση καλειται απο την εκφραση `inverted_index = create_inverted_index(cleaned_text)` που ειναι μετα την συναρτηση(στην αρχη γινεται η δηλωση της, και μετα εκτελειται απο αυτο).\n",
    "\n",
    "Οταν καλειται δεχεται ως παραμετρο το **documents** το οποιο ειναι στην ουσια η λιστα **cleaned_text** που δημιουργηθηκε μετα το tokenization που καναμε πιο πανω...δηλαδη το ***documents => cleaned_text => λιστα tokens( της συναρτησης preprocess_text )*** που εχει γινει ολη η διαδικασια του tokenization\n",
    "\n",
    "1) `inverted_index = defaultdict(list)`\n",
    "\n",
    "Για να δημιουργήσουμε ένα inverted index στην πραγματικότητα θα πρέπει να φτιάξουμε μία δομή δεδομένων που λειτουργεί ως λεξικό. \n",
    "\n",
    "**Δεν υπάρχει ήδη ετοιμη κλαση που να λειτουργεί ως inverted index**\n",
    "\n",
    "Η defaultdict  κατάλληλα για αυτή την περίπτωση  διότι μας επιτρέπει  να φτιάξουμε μία δομή δεδομένων  που δέχεται λέξεις(  δηλαδή ένα λεξικό ) και για κάθε λέξη,  να αποθηκεύουμε πληροφορίες για αυτήν.\n",
    "\n",
    "1. list in ο τύπος δεδομένων που θα αποθηκεύει η python( χρησιμοποίησει  η python  για κάθε λέξη )\n",
    "2. defaultdict  είναι  μία ειδική κλάση της python που αντιπροσωπεύει λεξικό(  δηλαδή μία δομή δεδομένων που σε αυτή μπορούμε να αποθηκεύσουμε λέξεις )\n",
    "Επομένως με αυτά τα δύο έχουμε φτιάξει μία δομή δεδομένων  που δέχεται λέξεις,  και για κάθε λέξη μπορούμε να έχουμε μία λίστα  που την αντιπροσωπεύει\n",
    "\n",
    "Η defaultdict  έχει  την ικανότητα να φτιάχνει αυτή  κενές λίστες  για κάθε λέξη(  στοιχείο )  που θα εισάγουμε σε αυτήν και αυτό θα μας βοηθήσει ιδιαίτερα,  διότι σε ένα πρώτο στάδιο φτιάχνει αυτόματα  τον inverted index  από μόνη της. \n",
    "\n",
    "2) `for doc_id, tokens in enumerate(documents):`\n",
    "Το **enumerate(documents)** μαζι με την for διατρέχει όλα τα έγγραφα στη λίστα documents και διαβαζει/επιστρεφει σε ποιο indexing βρισκομαστε( της λιστας documents ) και το περιεχομενο του εγγραφου.\n",
    "\n",
    "Η λίστα documents θυμίζω ότι έχει σε κάθε θεση της,  έχει τα καθαρισμένα tokens\n",
    "\n",
    "Επιστρέφεται ένα tuple τιμων:\n",
    "1. Η τιμη του στοιχειου(indexing) της λιστας που επεξεργάζεται εκείνη την ώρα...αποθηκευεται στο doc_id\n",
    "2. Tο περιεχόμενο της λιστας(  που είναι στην ουσία τα καθαρισμένα tokens )....**αποθηκευεται στην τοπικη λιστα tokens**\n",
    "\n",
    "2) `for word in set(tokens):`\n",
    "Φτιάχνουμε μία set λίστα όπου θα έχει περιεχόμενο όλα τα tokens(  που σημαίνει ότι δεν μπορούν να υπάρχουν και διπλότυπα )  και περιηγούμαστε σε αυτή  χρησιμοποιώντας μία for loop. \n",
    "   \n",
    "3) `inverted_index[word].append(doc_id)`\n",
    "Εφόσον η for loop  περγείται κάθε  token της λιστας set( που είναι στην ουσία η λίστα documents),  μπορούμε να  εκμεταλλευτούμε την ιδιότητα της defaultdict (  που είναι να προσθετει τα στοιχεία που ψάχνουμε όταν δεν υπάρχουν ηδη μέσα στη λίστα )  και παράλληλα να κάνουμε append σε καθε ενα απο αυτά  την  θέση του εγγράφου που βρίσκονται(doc_id)\n",
    "\n",
    "\n",
    "Ετσι φτιάχνουμε αυτόματα (  με αυτόν τον έξυπνο τρόπο  που λειτουργεί η defaultdict)  το inverted index  και παράλληλα  προσθέτουμε στα  keywords  τι θέση που βρίσκονται στην λίστα documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83911c6-8773-4a33-8663-327c81804657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_index(documents):\n",
    "    inverted_index = defaultdict(list)  # Κάθε όρος αντιστοιχεί σε λίστα εγγράφων\n",
    "    for doc_id, tokens in enumerate(documents):\n",
    "        \n",
    "        # Εισαγωγή λέξεων στη δομή\n",
    "        for word in set(tokens):  # Χρησιμοποιούμε set για να αποφύγουμε επαναλήψεις\n",
    "            inverted_index[word].append(doc_id)\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d9b074-f422-4453-a7ce-ec608ef42784",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = create_inverted_index(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cac944-6652-4ba5-bdb3-0fad19dd1382",
   "metadata": {},
   "source": [
    "1) `inverted_index.items()`\n",
    "Επιστρέφει όλα τα ζευγάρια κλειδιών και τιμών (key-value pairs) από το λεξικό inverted_index.\n",
    "\n",
    "2) `list(inverted_index.items())[:5]`\n",
    "Μετατρέπει τα αντικείμενα που επιστρέφει το inverted_index.items() σε λίστα και με το [:5], παίρνει τα πρώτα 5 στοιχεία από αυτή τη λίστα.\n",
    "Αν το inverted_index παραπάνω απο 5 λέξεις τοτε η εντολή [:5] θα περιορίσει την επανάληψη στις πρώτες 5 λέξεις.\n",
    "\n",
    "3) `print(f\"{word}: {doc_ids}\")`\n",
    "Χρησιμοποιεί τη μορφοποίηση f-string της Python για να εμφανίσει τη λέξη και τα IDs των εγγράφων όπου εμφανίζεται."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d57b93-0cac-428b-9e0b-7854f4e0d208",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, doc_ids in list(inverted_index.items())[:5]:\n",
    "    print(f\"{word}: {doc_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33de60ea-58f7-4438-9bbb-69ba9893a3c7",
   "metadata": {},
   "source": [
    "# ***Importing βιβλιοθηκης json***\n",
    "\n",
    "Αρχικα Κανουμε import την βιβλιοθηκη json ( επεξηγηση για το τι ειναι, υπαρχει στην αναφορα )\n",
    "\n",
    "1) `open(\"inverted_index.json\", \"w\", encoding=\"utf-8\")`\n",
    "Ανοίγει (ή δημιουργεί αν δεν υπαρχει ηδη) ένα αρχείο με το όνομα inverted_index.json στη λειτουργία write ( δηλαδη μπορουμε να το επεξεργαστουμε )\n",
    "\n",
    "2) `json.dump()`\n",
    "Αποθηκεύει δεδομένα σε μορφή JSON σε ένα αρχείο.\n",
    "\n",
    "3) `ensure_ascii=False`\n",
    "Διασφαλίζει ότι τα μη-ASCII δεδομένα θα αποθηκευτούν σωστά, αντί να κωδικοποιηθούν σε χαρακτήρες Unicode\n",
    "\n",
    "4) `indent=4`\n",
    "Κανουμε πιο ευαναγνστα τα δεδομενα, βαζοντας κενα πριν απο καθε δεδομενα( συγκεκριμενα εδω βαζουμε 4 κενα )\n",
    "\n",
    "\n",
    "To αρχειο ανοιγει με την ονομασια f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e357819d-da87-4494-8a56-5598f6b0bcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Save the inverted index to a JSON file\n",
    "with open(\"inverted_index.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(inverted_index, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c535d1ec-d18e-4c30-9090-4eebea7b070f",
   "metadata": {},
   "source": [
    "# ***Χρηση Inverted Index*** \n",
    "\n",
    "Αρχικα ανοιγουμε το αρχειο σε λειτουργεια read με `encoding=\"utf-8` ετσι ωστε να μπορουν να διαβαζονται σωστα οι ascii χαρακτηρες\n",
    "Το αρχειο ανοιγει με την ονομασια f\n",
    "\n",
    "`inverted_index = json.load(f)`\n",
    "Φορτώνει τα δεδομένα JSON από το αρχείο f και μετατρεπει το αρχειο json σε μια δομή δεδομένων της Python\n",
    "1. Αν το JSON είναι ένα λεξικό =>  γίνεται dict\n",
    "2. Αν το JSON είναι μια λίστα => γίνεται list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c61f7d8-3297-40e1-aaea-3d66969d41ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"inverted_index.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    inverted_index = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227d1919-164f-42de-b71a-95f126b5ed4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = \"wikipedia_data.csv\"\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "data = pd.read_csv(csv_file_path, header=0)    \n",
    "#it has the urls of the docs\n",
    "urls = data['URL'].tolist() \n",
    "documents = data['Text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bef7b54-cbec-4ebb-8e1c-4f9b71c080d2",
   "metadata": {},
   "source": [
    "# ***Δηλωση της συναρτησης preprocess_query και λειτουργεια της***\n",
    "**πολλα βηματα εδω εχουν εξηγηθει με παρομοιο τροπο προηγουμενως**\n",
    "\n",
    "Αυτη η συναρτηση επεξεργαζεται ενα query\n",
    "\n",
    "1) Αρχικα δημιουργει ενα **set αντικειμενο** με τα stopwords της αγγλικης γλωσσας( οπως ειχα κανει/εξηγησει και παραπανω )\n",
    "2) Επειτα στην λιστα των stop words αφαιρουμε τις λεξεις:\n",
    "   1. **and**\n",
    "   2. **or**\n",
    "   3. **not**\n",
    "\n",
    "3) Κανουμε απλο tokenize το query\n",
    "4) Μετατρεπουμε καθε λεξη σε πεζα γραμματα ( με `token.lower()`) και αφαιρουμε οτι δεν ειναι γραμμα ( λογω του regex του )\n",
    "5) Τελος στο tokenized query, το περναμε απο μια for Loop και αφαιρουμε τις λεξεις που **ΠΛΕΟΝ θεωρουνται stop_words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1447af53-fd1d-40d4-9cd6-67a900684b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query(query):\n",
    "    stop_words = set(stopwords.words(\"english\")) \n",
    "    #it should not remove AND OR NOT\n",
    "    stop_words = [word for word in stop_words if word not in ['and', 'or', 'not']]\n",
    "    tokens = word_tokenize(query)\n",
    "    tokens = [re.sub(r\"[^a-zA-Z]\", \"\", token.lower()) for token in tokens]\n",
    "    tokens = [word for word in tokens if word and word not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e69b761-73ae-4a73-8606-3abcabbadd53",
   "metadata": {},
   "source": [
    "# ***Δηλωση των συναρτησεων and,or,not***\n",
    "\n",
    "Πως λειτουργει η boolean retrival\n",
    "\n",
    "Το posting 1 και 2 ειναι λιστες αριθμων. Πιο συγκεκριμενα\n",
    "1. posting 1: Eίναι η λίστα η περιέχει το συνολικό αριθμό των εγγράφων( δηλαδή μία λίστα ακεραιων οπου κάθε θέση της ειναι ενα εγγραφο )\n",
    "2. posting 2: Είναι η λίστα που περιεχει τα εγγραφα( σαν αριθμους ) που εμφανιστηκε το token που θελουμε να επεξεργαστουμε\n",
    "\n",
    "Αρα ειναι 2 λιστες ακεραιων και η boolean Λογικη δουλευει ως εξης: \n",
    "\n",
    "### AND\n",
    "\n",
    "Στην λογική πράξη AND,  το αποτέλεσμα  είναι μία τελική λίστα  όπου  θα έχει ως περιεχόμενο τους αριθμούς  που είναι κοινοι και στις 2 λιστες.\n",
    "\n",
    "\n",
    "### OR \n",
    "\n",
    "Συνδυάζονται τα στοιχεία από δύο σύνολα, χωρίς διπλότυπα.\n",
    "Διοτι η OR σε μια τετοια περιπτωση θα δεχεται τα παντα\n",
    "\n",
    "\n",
    "### NOT \n",
    "\n",
    "Αφαιρούνται από το πρώτο σύνολο τα στοιχεία του δεύτερου."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbe6c44-26b4-46dc-a88c-6c16b422dd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_and(postings1, postings2):\n",
    "    return list(set(postings1) & set(postings2))\n",
    "\n",
    "def boolean_or(postings1, postings2):\n",
    "    return list(set(postings1) | set(postings2))\n",
    "\n",
    "def boolean_not(postings, total_docs):\n",
    "    return list(set(total_docs) - set(postings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff27cb6-2f86-431e-ab1a-8cd009544352",
   "metadata": {},
   "source": [
    "# ***Δηλωση της search και η λειτουργεια της***\n",
    " \n",
    "### query\n",
    "Προκειται για ένα query το οποίο έχει δοθεί  από εμένα ως `query = \"crawler AND bot\"` ΑΡΓΟΤΕΡΑ ΣΤΟΝ ΚΩΔΙΚΑ\n",
    "ΔΙΟΤΙ ΠΡΩΤΑ ΔΗΛΩΝΩ ΟΛΕΣ ΤΙΣ ΣΥΝΑΡΤΗΣΕΙΣ\n",
    "θα λειτουργησουν συνεργατικα\n",
    "\n",
    "### inverted_index\n",
    "Η δομη inverted index που έχει δημιουργηθεί προηγουμένως από τον κώδικα\n",
    "\n",
    "### total_docs\n",
    "Παιρνει την τιμη της με βαση αυτη την εντολη  `total_docs = data.shape[0]`, η οποια επιστρεφει το πληθος των γραμμων στο dataframe που ειχαμε φτιαξει.\n",
    "\n",
    "Ο λόγος που συμβαίνει αυτό είναι γιατί  κάνουμε χρήση της βιβλιοθήκης pandas  και πιο συγκεκριμένα της μεθόδου shape  η οποια επιστρέφει ένα tuple τιμων.\n",
    "\n",
    "1. shape[0] Ο αριθμός των γραμμών μιας λιστας \n",
    "2. shape[1] Ο αριθμός των στηλών μιας λιστας\n",
    "\n",
    "Επομένως εφοσον κάνουμε χρήση αυτής της μεθόδου  στην λίστα data και επιλέγουμε να πάρουμε το πρώτο στοιχείο του tuple( shape[0] ),  τότε θα μας επιστραφεί  ο συνολικός αριθμός των γραμμών του dataframe\n",
    "\n",
    "## `query_tokens = preprocess_query(query)`\n",
    "\n",
    "Παίρνουμε το προηγούμενο query και το περνάμε από διαδικασια tokenization, αποθηκεύοντας το αποτέλεσμα( λιστα )  στη μεταβλητή query_tokens\n",
    "\n",
    "\n",
    "## `results = set(range(total_docs))`\n",
    "\n",
    "Η συνάρτηση range δημιουργεί μία ακολουθία αριθμών  και δέχεται ως όρισμα έναν ακέραιο...γενικα  σαν συνάρτηση δέχεται μόνο ακεραιους). Κανουμε χρηση μονο της λειτουργιας της πρωτης παραμετρου της( start ) διοτι αυτη η συναρτηση εχει 3 παραμετρους.\n",
    "\n",
    "Ο ακέραιος που δέχεται ως ορισμα  είναι ο αριθμός των γραμμών του data frame που είχαμε δημιουργήσει( ακριβως απο πανω εξηγω γιατι )\n",
    "\n",
    "Τέλος φτιάχνουμε μία λίστα set(  έχω εξηγήσει πολλές φορές τι ειναι αυτο μεχρι τωρα )  έτσι ώστε να έχουμε μία λίστα με τον συνολικό αριθμό των εγγράφων χωρις διπλοτυπα.\n",
    "\n",
    "## `operation = \"AND\"`\n",
    "\n",
    "Επιλεγω αρχικα την boolean τιμή \"and\" ( για να δηλωσω και την μεταβλητη operation ως boolean...αλλα αναλογως το query μας, αυτο μπορει να μην εχει ιδιατερη σημασια )\n",
    "\n",
    "## Επεξηγηση Loop\n",
    "\n",
    "Aφού έχουμε δημιουργήσει την λίστα query_tokens(  που έχει προέλθει από το αρχικό μας query με tokenization ),  κάνουμε προσπέλαση σε κάθε στοιχείο αυτής της λίστας χρησιμοποιώντας μία for loop.\n",
    "\n",
    "Mε τη χρήση της μεθόδου `upper` μετατρέπουμε με το token  της λιστας που επεξεργαζόμαστε εκείνη την στιγμή σε κεφαλαια,  και ελέγχουμε  αν είναι  κάποια από τις τιμές \"AND\" \"OR\" \"NOT\" \n",
    "\n",
    "\n",
    "**Αν ειναι τοτε,  το αποθηκεύουμε στην μεταβλητη operation  έτσι ώστε να το χρησιμοποιήσουμε αργότερα.** \n",
    "\n",
    "**Αν δεν είναι αναζητάμε στο inverted index αυτό το token  και την λίστα του.**\n",
    "\n",
    "\n",
    "Τελος αναλόγως της τιμης του operation,  θα καλέσουμε και την αντίστοιχη συνάρτηση για boolean retrieval.\n",
    "\n",
    "**Η λογικη της boolean retrival, την εξηγω απο πανω στο δικο της μερος**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea94179-bf81-421b-a924-037695187b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, inverted_index, total_docs):\n",
    "    # Χωρισμός σε λέξεις και προεπεξεργασία\n",
    "    query_tokens = preprocess_query(query)\n",
    "    \n",
    "    # Ανάλυση ερωτήματος για Boolean λειτουργίες\n",
    "    results = set(range(total_docs))  # Όλα τα έγγραφα αρχικά\n",
    "    operation = \"AND\"  # Default λειτουργία\n",
    "    \n",
    "    for token in query_tokens:\n",
    "        if token.upper() in [\"AND\", \"OR\", \"NOT\"]:\n",
    "            operation = token.upper()\n",
    "        else:\n",
    "            postings = inverted_index.get(token, [])\n",
    "            \n",
    "            if operation == \"AND\":\n",
    "                results = boolean_and(results, postings)\n",
    "            elif operation == \"OR\":\n",
    "                results = boolean_or(results, postings)\n",
    "            elif operation == \"NOT\":\n",
    "                results = boolean_not(postings, range(total_docs))\n",
    "    \n",
    "    return sorted(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25201e46-b8f7-4d2b-8f2b-78bc03832588",
   "metadata": {},
   "source": [
    "# ***Δηλωση της compute_tf και η λειτουργεια της***\n",
    "\n",
    "Η συνάρτηση compute_tf υπολογίζει τη συχνότητα όρου (Term Frequency - TF) μιας συγκεκριμένης λέξης μέσα σε ένα έγγραφο \n",
    "Εδω θα υπολογισουμε ποσες φορες εμφανιζεται μια λεξη στις παγραφους ενος html αρχειου( αυτο θα γινει για καθε html αρχειο ) και θα το συγκρινουμε με το συνολικο πληθος των λεξεων του html \n",
    "\n",
    "Ετσι θα βγαλουμε την μαθηματικη σχεση της συχνοτητας\n",
    "\n",
    "## Παραμετροι \n",
    "\n",
    "`doc`: Ειναι το documents[doc_id] που καλειται αργοτερα\n",
    "\n",
    "Θυμιζω οτι η λιστα documents εχει δημιουργηθει απο το dataframe των παραγραφων του καθε html αρχειου οπου καθε θεση της λιστας περιεχει ολες τις παραγραφους ενος html αρχειου.\n",
    "\n",
    "`word`: Η λέξη για την οποία θέλουμε να υπολογίσουμε τη συχνότητα εμφάνισης στο κείμενο του εγγράφου \n",
    "\n",
    "1) `doc.split()`: Διαχωρίζει τις παραγραφους( που εχει το dataframe για ενα συγκεκριμενο html) σε μια λίστα από λέξεις \n",
    "2) `words.count(word)`: Μετράει πόσες φορές εμφανίζεται η λέξη word στη λίστα words( η οποια ειναι το αποτελεσμα της doc.split απο πανω )\n",
    "3) `len(words)`: Υπολογίζει τον συνολικό αριθμό των λέξεων στη λίστα words.\n",
    "4) `word_count / total_words`: Μαθηματικη εκφραση που υπλογιζει την συχνοτητα εμφανισης της λεξης"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e689e3d-de5b-4214-8e12-631726ddbfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(doc, word):\n",
    "    words = doc.split()\n",
    "    word_count = words.count(word)\n",
    "    total_words = len(words)\n",
    "    tf = word_count / total_words\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979c3c5c-2020-4288-841e-f22c84f76c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f319cb59-4bc1-4fc4-aaf2-8bcfd3c376b1",
   "metadata": {},
   "source": [
    "# ***Δηλωση της compute_idf και η λειτουργεια της***\n",
    "\n",
    "Η συνάρτηση compute_idf υπολογίζει την αντίστροφη συχνότητα εγγράφου ( Inverse Document Frequency ) μιας λέξης \n",
    "\n",
    "Δηλαδη μετράει πόσο \"σπάνια\" είναι μια λέξη σε ολόκληρη τη συλλογή εγγράφων\n",
    "\n",
    "1) `inverted_index.get(word, [])`: Βρίσκει τη λίστα των εγγράφων που περιέχουν τη λέξη word.\n",
    "Επειτα ελέγχουμε αν η λεξη δεν εμφανιζεται σε κανεναν εγγραφο, και αν ισχυει επιστρεφουμε την τιμη 0...διαφορετικα υπολογιζουμε την τιμη "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6b7011-ad3c-4daa-ac53-1ab89cb0fef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(inverted_index, word, total_docs):\n",
    "    doc_freq = len(inverted_index.get(word, []))\n",
    "    if doc_freq == 0:\n",
    "        return 0\n",
    "    idf = math.log(total_docs / doc_freq)\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425d00b5-4083-4c33-8d0c-50634212eb62",
   "metadata": {},
   "source": [
    "# ***Δηλωση της compute_tfidf και η λειτουργεια της***\n",
    "\n",
    "Η συνάρτηση compute_tfidf υπολογίζει τη συχνότητα όρου-αντίστροφη συχνότητα εγγράφου (TF-IDF) μιας λέξης (word) σε ένα συγκεκριμένο έγγραφο (doc_id)\n",
    "Για τον υπολογισμο του, συνδυαζονται οι τιμες του tf και idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b84f6c5-7b12-410b-bebd-e45c863bf835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf(doc_id, word, documents, inverted_index):\n",
    "    total_docs = len(documents)\n",
    "    \n",
    "    # Υπολογισμός TF για το έγγραφο\n",
    "    tf = compute_tf(documents[doc_id], word)\n",
    "    \n",
    "    # Υπολογισμός IDF για τη λέξη\n",
    "    idf = compute_idf(inverted_index, word, total_docs)\n",
    "    \n",
    "    # Υπολογισμός TF-IDF\n",
    "    tfidf = tf * idf\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef29ad9-bccd-4f83-b2cd-f740f3550fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1a5ea7-d029-4459-a7fd-9ad2e92ef868",
   "metadata": {},
   "source": [
    "# ***Δηλωση της search_tf_idf και η λειτουργεια της***\n",
    "\n",
    "Η συνάρτηση search_tf_idf εκτελεί αναζήτηση TF-IDF σε μια συλλογή εγγράφων για ένα συγκεκριμένο ερώτημα\n",
    "Χρησιμοποιεί τη συνάρτηση search για να επιστρέψει τα σχετικά έγγραφα (τα έγγραφα που περιέχουν τις λέξεις του ερωτήματος)\n",
    "\n",
    "Xρησιμοποιεί τη συνάρτηση preprocess_query για να επεξεργαστεί το ερώτημα και να το μετατρέψει σε tokens, αφαιρωντας stop_words εκτος των boolean λεξεων που βγαλαμε απο το συνολο των stopwords\n",
    "\n",
    "Επειτα για καθε λεξη αν δεν ειναι boolean, τοτε Υπολογίζει το TF-IDF score της λέξης για κάθε έγγραφο στα result_docs και αποθηκεύει τα TF-IDF scores της λέξης στη λίστα scores\n",
    "\n",
    "Μετατρέπει το q_scores σε NumPy array για εύκολους υπολογισμούς\n",
    "Υπολογίζει τον μέσο όρο (mean) των TF-IDF scores κάθε λέξης για κάθε έγγραφο"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c35814b-9a6c-4e3e-927e-8457bc02ebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tf_idf(query, inverted_index, documents):\n",
    "    total_docs = len(documents)\n",
    "    result_docs = search(query, inverted_index, total_docs)\n",
    "\n",
    "    query_tokens = preprocess_query(query)\n",
    "    q_scores = []\n",
    "    for q in query_tokens:\n",
    "        if q not in ['and', 'or', 'not']:\n",
    "            scores = []\n",
    "            for i in result_docs:\n",
    "                scores.append(compute_tfidf(i,q,documents,inverted_index))\n",
    "            q_scores.append(scores)\n",
    "    \n",
    "    q_scores = np.array(q_scores)        \n",
    "    final_scores = np.mean(q_scores,axis=0)\n",
    "    \n",
    "    \n",
    "    sorted_terms, sorted_scores = zip(*sorted(zip(result_docs, final_scores), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    # Convert back to lists \n",
    "    sorted_terms = list(sorted_terms)\n",
    "    sorted_scores = list(sorted_scores)\n",
    "    return(sorted_terms, sorted_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d19a3e-e245-4816-a96b-c672d537b158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113fbf51-0879-4a88-bac9-5cfb71eb1ee4",
   "metadata": {},
   "source": [
    "# ***Δηλωση της vector_space_retrieve και η λειτουργεια της***\n",
    "\n",
    "**στην αναφορά εξηγώ εκτενως τι είναι το VSR μοντελο και πως λειτουργει**\n",
    "\n",
    "## Παραμετροι \n",
    "\n",
    "`query`: Η ερωτηση...δηλαδη αυτο που θελω να ψαξω στα εγγραφα\n",
    "`inverted_index`: Το ανάστροφο ευρετήριο απο πριν\n",
    "`documents`: Καθε θεση αυτης της λιστας εχει ολες τις παραγραφους του καθε html αρχειου αντιστοιχα\n",
    "\n",
    "## Εντολες \n",
    "\n",
    "1. `total_docs = len(documents)`: Επιστρεφει το πληθος των στοιχειων της λιστας documents \n",
    "2. `result_docs = search(query, inverted_index, total_docs)`: Εκμεταλευομαστε την συναρτηση search διοτι θα μας επιστρεψει ποια εγγραφα περιεχουν τις λεξεις του query\n",
    "\n",
    "3. `vectorizer = TfidfVectorizer()`: μετατρέπει κείμενο (έγγραφα) σε μαθηματικά διανύσματα TF-IDF χρησιμοποιωντας την βιβλιοθήκη scikit-learn\n",
    "4. `query_vector = vectorizer.transform([query])` : μετατρεπει το query( ερωτηση ) σε διανυσμα χρησιμοποιωντας την βιβλιοθήκη scikit-learn\n",
    "5. `query_tfidf = query_vector.toarray()`: μετατρεπει το διανυσμα του query σε πινακα array ( γιατι απο μονο του θα ηταν μια τιμη tf idf )\n",
    "6. `document_vectors = tfidf_matrix[result_docs]` : επιλέγει τα TF-IDF διανύσματα μόνο για τα έγγραφα που είναι υποψήφια για σύγκριση, δηλαδή αυτά που επέστρεψε η search ως σχετικά με το query.\n",
    "7. `similarities = cosine_similarity(query_tfidf, document_vectors)`: υπολογίζει την cosine similarity του διανύσματος της ερώτησης και των διανυσμάτων των εγγράφων.\n",
    "\n",
    "Στο τελος ταξινομουμε τα αποτελέσματα της cosine similarity με φθίνουσα σειρά, συνδυάζοντας τα έγγραφα με τις βαθμολογίες συσχέτισης (similarities[0]). Τελικά, επιστρέφει δύο ξεχωριστές λίστες:\n",
    "\n",
    "1. Τα IDs των εγγράφων (sorted_terms).\n",
    "2. Τις αντίστοιχες βαθμολογίες cosine similarity (sorted_scores).\n",
    "\n",
    "Στο τελος μετατρεπουμε το tuple τιμων που επιστεψε η zip, πισω σε μορφη λιστας"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb18caba-cc48-4a5f-bd12-4cc3cb980cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_space_retrieve(query, inverted_index, documents):\n",
    "    total_docs = len(documents)\n",
    "    result_docs = search(query, inverted_index, total_docs)\n",
    "    \n",
    "    \n",
    "    # Calculate the cosine similarity between the query and the documents\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    \n",
    "    \n",
    "    # Get the TF-IDF vector of the query (it will be a sparse matrix)\n",
    "    query_tfidf = query_vector.toarray()\n",
    "\n",
    "    # Compute cosine similarity between the query and all documents in the result set\n",
    "    document_vectors = tfidf_matrix[result_docs]\n",
    "    \n",
    "    # Compute cosine similarity of query with each document in the result set\n",
    "    similarities = cosine_similarity(query_tfidf, document_vectors)\n",
    "    \n",
    "    \n",
    "    sorted_terms, sorted_scores = zip(*sorted(zip(result_docs, list(similarities[0])), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    # Convert back to lists \n",
    "    sorted_terms = list(sorted_terms)\n",
    "    sorted_scores = list(sorted_scores)\n",
    "    \n",
    "    return(sorted_terms, sorted_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a639bc-2b3a-476a-a2ee-edd3301cfc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707ea009-4498-45e0-aede-6a0d4ee85c9a",
   "metadata": {},
   "source": [
    "### ***ΕΔΩ ΜΕ ΕΠΙΑΣΕ ΤΕΝΟΝΤΙΤΙΔΑ...ΟΠΩΣ ΓΡΑΦΩ ΣΤΟ WORD ΥΠΟΦΕΡΩ ΑΠΟ ΜΕΓΑΛΟ ΘΕΜΑ ΜΕ ΚΗΛΗ ΣΤΟ ΑΥΧΕΝΑ + ΚΑΡΠΙΑΙΟ***\n",
    "ΔΕΝ ΜΠΟΡΩ ΝΑ ΓΡΑΨΩ ΑΛΛΟ ΤΗΝ ΕΠΕΞΗΓΗΣΗ ΑΠΟ ΕΔΩ ΚΑΙ ΠΕΡΑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8b7154-34b9-4192-a820-89b93d36fbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inverted_index_freq(documents):\n",
    "        inverted_index = {}\n",
    "        for doc_id, doc in enumerate(documents):\n",
    "            terms = doc.split()\n",
    "            term_freq = Counter(terms)\n",
    "            for term, freq in term_freq.items():\n",
    "                if term not in inverted_index:\n",
    "                    inverted_index[term] = []\n",
    "                inverted_index[term].append((doc_id, freq))\n",
    "        return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c9f202-2752-466e-9946-2dccb1dc17d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bm25(query, inverted_index, documents):\n",
    "    #params\n",
    "    k1=1.5\n",
    "    b=0.75\n",
    "    \n",
    "    doc_lengths = [len(doc.split()) for doc in documents]\n",
    "    avg_doc_length = sum(doc_lengths) / len(documents)\n",
    "    \n",
    "    #calc IDF\n",
    "    idf = {}\n",
    "    total_docs = len(documents)\n",
    "    for term in inverted_index:\n",
    "        doc_freq = len(inverted_index[term])\n",
    "        idf[term] = math.log((total_docs - doc_freq + 0.5) / (doc_freq + 0.5) + 1.0)\n",
    "    \n",
    "    total_docs = len(documents)\n",
    "    result_docs = search(query, inverted_index, total_docs)\n",
    "    \n",
    "    scores = [0] * len(result_docs)  # Score for each document\n",
    "    query_terms = preprocess_query(query)\n",
    "    for term in query_terms:\n",
    "        if term not in ['and', 'or', 'not']:\n",
    "            if term in inverted_index:\n",
    "                idf_t = idf.get(term, 0)\n",
    "                for i, doc_id in enumerate(result_docs):\n",
    "                    word_list = documents[doc_id].split()\n",
    "                    term_freq = word_list.count(term)\n",
    "                    doc_length = doc_lengths[doc_id]\n",
    "                    tf = term_freq\n",
    "                    score = idf_t * tf * (k1 + 1) / (tf + k1 * (1 - b + b * doc_length / avg_doc_length))\n",
    "                    scores[i] += score\n",
    "                    \n",
    "    sorted_terms, sorted_scores = zip(*sorted(zip(result_docs, scores), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    # Convert back to lists \n",
    "    sorted_terms = list(sorted_terms)\n",
    "    sorted_scores = list(sorted_scores)\n",
    "    \n",
    "    return(sorted_terms, sorted_scores)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5d1526-f5f4-42f5-8dcc-1f2b3bdd6ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"crawler AND bot\"\n",
    "total_docs = data.shape[0]  # Συνολικός αριθμός εγγράφων\n",
    "\n",
    "# Αναζήτηση Boolean Retrieval\n",
    "result_docs = search(query, inverted_index, total_docs)\n",
    "print(\"Αποτελέσματα Boolean Retrieval:\")\n",
    "for i in result_docs:\n",
    "    print(urls[i])\n",
    "\n",
    "# Αναζήτηση TF-IDF\n",
    "sorted_terms, sorted_scores = search_tf_idf(query, inverted_index, documents)\n",
    "\n",
    "print(\"Αποτελέσματα TF-IDF:\")\n",
    "for i in range(len(sorted_terms)):\n",
    "    print(\"%s \\t %.6f\" %( urls[sorted_terms[i]], sorted_scores[i]))\n",
    "\n",
    "# Αναζήτηση vector space retrieve\n",
    "sorted_terms, sorted_scores = vector_space_retrieve(query, inverted_index, documents)\n",
    "\n",
    "print(\"Αποτελέσματα vector space retrieve:\")\n",
    "for i in range(len(sorted_terms)):\n",
    "    print(\"%s  \\t %.6f\" %( urls[sorted_terms[i]], sorted_scores[i]))\n",
    "\n",
    "\n",
    "# Αναζήτηση bm25\n",
    "sorted_terms, sorted_scores = calculate_bm25(query, inverted_index, documents)\n",
    "\n",
    "print(\"Αποτελέσματα BM25:\")\n",
    "for i in range(len(sorted_terms)):\n",
    "    print(\"%s  \\t %.6f\" %( urls[sorted_terms[i]], sorted_scores[i]))\n",
    "    \n",
    "\n",
    "# EVALUATION - USE NEWSGROUP DATA\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score\n",
    "\n",
    "def scores(retrieved_results, relevant_results, M):\n",
    "    # Δημιουργία των δυαδικών λιστών (true/false) για τα αποτελέσματα\n",
    "    # Αν το έγγραφο είναι σχετικό, το σημειώνουμε ως 1, αλλιώς ως 0\n",
    "    y_true = [1 if doc in relevant_results else 0 for doc in range(M)]  # Σχετικά έγγραφα\n",
    "    y_pred = [1 if doc in retrieved_results else 0 for doc in range(M)]  # Επιστρεφόμενα έγγραφα\n",
    "\n",
    "    # Υπολογισμός των μετρικών\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    map_score = average_precision_score(y_true, y_pred)\n",
    "\n",
    "    # Εκτύπωση αποτελεσμάτων\n",
    "    print(f\"Ακρίβεια (Precision): {precision:.4f}\")\n",
    "    print(f\"Ανάκληση (Recall): {recall:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "    print(f\"Μέση Ακρίβεια (MAP): {map_score:.4f}\")\n",
    "    \n",
    "    \n",
    "# Load the dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "documents = newsgroups.data\n",
    "labels = newsgroups.target\n",
    "categories = newsgroups.target_names\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "query = \"space shuttle launch\"\n",
    "total_docs = len(documents)  # Συνολικός αριθμός εγγράφων\n",
    "relevant_results = np.where(labels == categories.index('sci.space'))\n",
    "relevant_results = list(relevant_results[0])    \n",
    "\n",
    "\n",
    "top_k = 10\n",
    "# Αναζήτηση Boolean Retrieval\n",
    "sorted_terms = search(query, inverted_index, total_docs)\n",
    "print(\"Αποτελέσματα Boolean Retrieval:\")\n",
    "for i in range(top_k):\n",
    "    print(sorted_terms[i])\n",
    "\n",
    "retrieved_results = list(sorted_terms[:top_k])\n",
    "scores(retrieved_results, relevant_results, total_docs)\n",
    "\n",
    "# Αναζήτηση TF-IDF\n",
    "sorted_terms, sorted_scores = search_tf_idf(query, inverted_index, documents)\n",
    "top_k = 10\n",
    "print(\"Αποτελέσματα TF-IDF:\")\n",
    "for i in range(top_k):\n",
    "    print(\"%s %.6f\" %( sorted_terms[i], sorted_scores[i]))\n",
    "\n",
    "retrieved_results = sorted_terms[:top_k]\n",
    "scores(retrieved_results, relevant_results, total_docs)\n",
    "\n",
    "# Αναζήτηση vector space retrieve\n",
    "sorted_terms, sorted_scores = vector_space_retrieve(query, inverted_index, documents)\n",
    "\n",
    "print(\"Αποτελέσματα vector space retrieve:\")\n",
    "for i in range(top_k):\n",
    "    print(\"%s %.6f\" %( sorted_terms[i], sorted_scores[i]))\n",
    "\n",
    "retrieved_results = sorted_terms[:top_k]\n",
    "scores(retrieved_results, relevant_results, total_docs)\n",
    "\n",
    "# Αναζήτηση bm25\n",
    "sorted_terms, sorted_scores = calculate_bm25(query, inverted_index, documents)\n",
    "\n",
    "print(\"Αποτελέσματα BM25:\")\n",
    "for i in range(top_k):\n",
    "    print(\"%s %.6f\" %( sorted_terms[i], sorted_scores[i]))\n",
    "\n",
    "retrieved_results = sorted_terms[:top_k]\n",
    "scores(retrieved_results, relevant_results, total_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
